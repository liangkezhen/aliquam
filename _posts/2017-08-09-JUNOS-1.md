---
layout: post
title:  协议杂谈 (一) Sockets
category: tech 
comments: true
description: 
tags:
    - private 
---




最近完成了CFM 协议的一个增强特性的开发，借此个机会，比较全面的研究了一下CFM 协议的框架代码，感觉收获还是挺大。由于JUNOS 所有控制平面Daemon 采用相同的架构，因此本文的出发点虽然只是CFM 协议，而架构的知识实际上也适用于其他控制平面协议。本系列文章的名字叫做协议杂谈，实在是因为作者虽有心以明确的框架，清晰的表述来写一个系列，但因为能力有限，精力有限，目前只能想到哪里写到哪里。 

### 数据平面与控制平面的分离

JUNOS 基于Freebsd 开发, 控制平面Daemon内的线程都亲和在cpu core 0 上，按照任务调度的方式，分时占用CPU。 所有控制平面的Daemon 组成了RE (routine engine)。与之对应的，数据平面Daemon,称之为FLOWD, 其又分为两部分，一部分线程负责与管理平面通信，仍亲和于core 0， 另一部分则是纯粹的转发线程，他们根据控制平面的指令维护转发表，并根据转发表转发报文。这部分线程亲和于core 1， core 2, core n ..... 由于负责转发的线程永远不会让出CPU, 可以保证了报文的实时转发。 对于这一部分线程的集合，我们称之为PFE(packet forwarding engine)。

由于历史原因，JUNOS 曾有两套协议栈，二层协议位于/src/esp-shared/usr.sbin/ 目录下，三层协议代码位于／src／junos/usr.sbin/ 目录之下。现在已经统一到相同的目录下了。在这个目录下，每个daemon 的代码位于以daemon 名命名的文件夹中，自成体系。以下是几个经常打交道的daemon的代码路径，如cfmd, lfmd, vrrpd，l2ald等等。

```
/src/junos/usr.sbin/cfmd
/src/junos/usr.sbin/lfmd
/src/junos/usr.sbin/vrrpd
/src/junos/usr.sbin/l2ald
```

在X48 的时代， RE daemon 是基于jtask lib 创建的，协议模块被分为很多功能子module,每个module 有自己的task。jtask lib 封装了 kqueue 作为事件响应机制，为每个task 创建了RTsocket 用于同kernel 同步信息。用户daemon 只要实现自己的message handler 即可，而不必关心背后的通信机制。

到了X49 的时代，kqueue 机制进化为eventlib。


###  Junos 上的socket 

RE 与 kernel 之间的通信是通过socket 实现的。
关于socket，网上有很多
[资料](http://blog.csdn.net/dlutbrucezhang/article/details/8577810)，不过一般搜索到的资料仅仅是TCP/IP 架构上的socket，即domain 为 inet，适用于client／server 架构。JUNOS 中定义的socket 种类复杂，用途各异。

简而言之，在Junos 中有这样的三种socket。

1.  AF_RAWIF 类型， 用于 Daemon 收发包
2.  AF_INET  类型， 用于TCP/IP 通信
3.  AF_ROUTE 类型， 用于与kernel 通信

和任何网络操作系统一样，JUNOS 也定义了标准的Socket 的系统调用，如下所示，但具体到不同的socket，系统调用的内部实现又得具体分析了。

```
int socket(int domain, int type, int protocol);   ／* 创建一个socket*／
int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen); ／*将socket 与addr 进行绑定*／
int listen(int sockfd, int backlog); ／*监听某个socket 上的连接请求*／
int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); ／*client 端socket 连接到指定addr 的socket 上*／
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen); ／*接受client 端的连接请求*／
int close(int fd);／* 关闭连接*／
ssize_t read(int fd, void *buf, size_t count);
ssize_t write(int fd, const void *buf, size_t count);
```

Junos 里这组系统调用的原型在这个文件里

```
bsd/sys/kern/uipc_socket.c
```

通常，socket 应用于client／server 通信模型下。 服务器端的进程创建自己的socket，将起与ip+port 关联起来（bind），然后监听（listen）来自client 的连接（connect）请求。如果是合法的请求，则接受（accept）该请求，双方开始数据通信（read／write）。当通信结束，关闭该连接（close）。

需要强调的是，socket 的通信是process 间的通信，所以他的address 不仅要包含主机信息，还要包含进程的信息，也就是ip+port.

```
junos/bsd/sys/netinet/in.h
 
  95 /* Socket address, internet style. */
  96 struct sockaddr_in {
  97     u_int8_t  sin_len;
  98     u_int8_t  sin_family;
  99     u_int16_t sin_port;
 100     struct    in_addr sin_addr;
 101     char      sin_zero[8];
 102 };

```

RE 与kernel 通信主要是要实现两个需求

1. 协议模块收发包.对二层协议来说，协议要正常运转，必须周期性的交换BPDU，这要求协议模块需要能收发报文；
2. 修改Kernel 的状态；协议模块需要响应外部事件，作出相应的改变。例如VRRP 模块要更新kernel 路由表，LFM 模块要更新ifd flag，使之进入loopback 状态。


RE 与 Kernel 之间通过socket通信模型与普通的client／server 模型不同。首先，kernel 不是server，也没有自己的socket，无法实现connect，accept 这种操作，也不需要。第二，有些二层协议模块，指定interface 发送协议报文，不需要ip+lport，这种socket 不是基于TCP/UDP。对这种奇怪的socket ,他们的系统调用是什么样的 ？ 又是如何与inet 类型socket 区分的呢？


当创建一个socket 时，需要传入参数 domain，proto。 对不同的domain 和proto，系统预先定义了与之对应的结构体，这样实现了不同类型的socket 具有自己定制的socket handler。以下是 domain 为AF_INET， proto 为tcp 的一个结构体例子.

```
124 struct protosw inetsw[] = {

145 {
146     .pr_type =      SOCK_STREAM,
147     .pr_domain =        &inetdomain,
148     .pr_protocol =      IPPROTO_TCP,
149     .pr_flags =     PR_CONNREQUIRED|PR_IMPLOPCL|PR_WANTRCVD,
150     .pr_input =     tcp_input,
151     .pr_ctlinput =      tcp_ctlinput,
152     .pr_ctloutput =     tcp_ctloutput,
153     .pr_init =      tcp_init,
154     .pr_slowtimo =      tcp_slowtimo,
155     .pr_drain =     tcp_drain,
156     .pr_usrreqs =       &tcp_usrreqs
157 },

```

在pr_usrreqs 中，可以定制socket api 对应的handler 实例。


```
struct pr_usrreqs tcp_usrreqs = {                                                         
    .pru_abort =        tcp_usr_abort,                                                    
    .pru_accept =       tcp_usr_accept,                                                   
    .pru_attach =       tcp_usr_attach,                                                   
    .pru_bind =     tcp_usr_bind,                                                         
    .pru_connect =      tcp_usr_connect,                                                  
    .pru_control =      in_control,                                                       
    .pru_detach =       tcp_usr_detach,                                                   
    .pru_disconnect =   tcp_usr_disconnect,                                               
    .pru_listen =       tcp_usr_listen,                                                   
    .pru_peeraddr =     in_getpeeraddr,                                                   
    .pru_rcvd =     tcp_usr_rcvd,                                                         
    .pru_rcvoob =       tcp_usr_rcvoob,                                                   
    .pru_send =     tcp_usr_send,                                                         
    .pru_shutdown =     tcp_usr_shutdown,                                                 
    .pru_sockaddr =     in_getsockaddr,                                                   
    .pru_sosetlabel =   in_pcbsosetlabel,                                                 
    .pru_close =        tcp_usr_close,                                                    
};                        
```

我们前面提到的用于二层协议daemon 收／发包的socket 属于 rawif 类型，它最主要的是需要实现rcvd 和 send。 对bind 这类的操作，它是不需要的，因此他的pru_bind 实际上是返回了不支持的标志。 同时，对不同的协议，他们虽然都是使用rawif socket 进行收发包，然而协议之间的报文处理还是有差异，这种差异通过创建socket 的另一个参数proto 进行了区分。

```
410 struct pr_usrreqs raw_if_usrreqs = {
 411     .pru_abort =        raw_if_usr_abort,
 412     .pru_accept =       raw_if_usr_accept,
 413     .pru_attach =       raw_if_usr_attach,
 414     .pru_bind =         raw_if_usr_bind,
 415     .pru_connect =      raw_if_usr_connect,
 416     .pru_connect2 =     pru_connect2_notsupp,
 417     .pru_control =      raw_if_usr_control,
 418     .pru_detach =       raw_if_usr_detach,
 419     .pru_disconnect =       raw_if_usr_disconnect,
 420     .pru_listen =       raw_if_usr_listen,
 421     .pru_peeraddr =     raw_if_usr_peeraddr,
 422     .pru_rcvd =         raw_if_usr_rcvd,
 423     .pru_rcvoob =       raw_if_usr_rcvoob,
 424     .pru_send =         raw_if_usr_send,
 425     .pru_sense =        raw_if_usr_sense,
 426     .pru_shutdown =     raw_if_usr_shutdown,
 427     .pru_sockaddr =     raw_if_usr_sockaddr,
 428     .pru_sosend =       sosend,
 429     .pru_soreceive =        soreceive,
 430     .pru_sopoll =       sopoll
 431 };
 432 
 
 
 507 static int
 508 raw_if_usr_bind(struct socket *so, struct sockaddr *nam, struct thread *td)
 509 {  
 510     return EOPNOTSUPP; 
 511 }  


```

对于socket 的分发这部分，我总结了一张图，列出了关键的全局变量和API.



<figure>
<img alt="image test" src="/resources/images/socket.bmp"/>
<figcaption>
<strong>Figure 1: </strong> socket in Junos
</figcaption>
</figure>



最后，Kernel 与 RE daemon 之间同步信息使用的是rtsocket, 因为要同步的信息种类很多，所以系统定义了不同的message。信息的接收者，需要根据message type 将其分发（dispatch）到合适的handler 中去处理。 上图中RE daemon 向kernel 发送rtsoket message 的部分比较清晰了，kernel 向RE sync 状态的情况又有不同，待以后再说吧。




###  to be continue
